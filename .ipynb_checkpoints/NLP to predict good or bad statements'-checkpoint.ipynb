{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28e7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stolen from here: https://towardsdatascience.com/text-classification-predicting-good-or-bad-statements-using-natural-language-processing-e3a4edb07118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162570e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mas tu não és feio :( @SavageFluxXx__</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@SamaraPaivas Você que pensa :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>te amo demais :( https://t.co/leUzS65WrG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nicko_donis lindo! :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@B_kirikihira Oi, tem sim! Visite nossos canai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  sentiment\n",
       "0              Mas tu não és feio :( @SavageFluxXx__          0\n",
       "1                    @SamaraPaivas Você que pensa :)          1\n",
       "2           te amo demais :( https://t.co/leUzS65WrG          0\n",
       "3                             @nicko_donis lindo! :)          1\n",
       "4  @B_kirikihira Oi, tem sim! Visite nossos canai...          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://gist.githubusercontent.com/johnidm/582cfeadd2bf418df4539c9422f824d2/raw/twitter-sentiment-pt-BR-md-2-l.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c50774",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6af5a04344d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "nlp = spacy.blank(\"pt\")\n",
    "REGX_USERNAME = r\"@[A-Za-z0-9$-_@.&+]+\"\n",
    "REGX_URL = r\"https?://[A-Za-z0-9./]+\"\n",
    "def preprocessing(text):\n",
    "  text = text.lower()\n",
    "  \n",
    "  text = re.sub(REGX_USERNAME, ' ', text)\n",
    "  text = re.sub(REGX_URL, ' ', text)\n",
    "  \n",
    "  emojis = {\n",
    "      ':)': 'emocaopositiva',\n",
    "      ':(': 'emocaonegativa'\n",
    "  }\n",
    "  \n",
    "  for e in emojis:\n",
    "    text = text.replace(e, emojis[e])\n",
    "  \n",
    "  tokens = [token.text for token in nlp(text)]\n",
    "  \n",
    "  tokens = [t for t in tokens if \n",
    "              t not in STOP_WORDS and \n",
    "              t not in string.punctuation and \n",
    "              len(t) > 3]\n",
    "  \n",
    "  tokens = [t for t in tokens if not t.isdigit()]\n",
    "    \n",
    "  return \" \".join(tokens)\n",
    "df[\"tweet_text_clean\"] = df[\"tweet_text\"].apply(preprocessing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79ce99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 20000 - Train:  15000 - Dev: 3000 - Test: 2000\n"
     ]
    }
   ],
   "source": [
    "dataset = list(df[[\"tweet_text_clean\", \"sentiment\"]].sample(frac=1).itertuples(index=False, name=None))\n",
    "train_data = dataset[:15000]\n",
    "dev_data = dataset[15000:18000]\n",
    "test_data = dataset[18000:]\n",
    "print(f\"Total: {len(dataset)} - Train:  {len(train_data)} - Dev: {len(dev_data)} - Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f708efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data, outfile):\n",
    "    db = spacy.tokens.DocBin()\n",
    "    docs = []\n",
    "    for doc, label in nlp.pipe(data, as_tuples=True):\n",
    "        doc.cats[\"POS\"] = label == 1\n",
    "        doc.cats[\"NEG\"] = label == 0\n",
    "        db.add(doc)\n",
    "    \n",
    "    db.to_disk(outfile)\n",
    "convert(train_data, \"./train.spacy\")\n",
    "convert(dev_data, \"./dev.spacy\")\n",
    "convert(test_data, \"./test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5205379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Generated config template specific for your use case\n",
      "- Language: pt\n",
      "- Pipeline: textcat\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config  --lang pt --pipeline textcat --optimize efficiency --force config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "835a7d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Created output directory: model\n",
      "[i] Saving to output directory: model\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['textcat']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ----------  ------\n",
      "  0       0          0.25       85.29    0.85\n",
      "  0     200         33.79       97.29    0.97\n",
      "  0     400         14.76       98.32    0.98\n",
      "  0     600          7.42       98.96    0.99\n",
      "  1     800          4.11       99.06    0.99\n",
      "  1    1000          2.93       98.99    0.99\n",
      "  2    1200          2.11       98.89    0.99\n",
      "  3    1400          1.72       98.93    0.99\n",
      "  4    1600          1.42       98.93    0.99\n",
      "  5    1800          1.22       98.96    0.99\n",
      "  7    2000          1.03       98.93    0.99\n",
      "  9    2200          0.92       98.96    0.99\n",
      " 11    2400          0.76       98.99    0.99\n",
      "[+] Saved pipeline to output directory\n",
      "model\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-24 19:32:33,882] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "[2023-04-24 19:32:34,132] [INFO] Set up nlp object from config\n",
      "[2023-04-24 19:32:34,138] [DEBUG] Loading corpus from path: dev.spacy\n",
      "[2023-04-24 19:32:34,138] [DEBUG] Loading corpus from path: train.spacy\n",
      "[2023-04-24 19:32:34,139] [INFO] Pipeline: ['textcat']\n",
      "[2023-04-24 19:32:34,141] [INFO] Created vocabulary\n",
      "[2023-04-24 19:32:34,141] [INFO] Finished initializing nlp object\n",
      "[2023-04-24 19:32:36,286] [INFO] Initialized pipeline components: ['textcat']\n",
      "[2023-04-24 19:32:36,294] [DEBUG] Loading corpus from path: dev.spacy\n",
      "[2023-04-24 19:32:36,294] [DEBUG] Loading corpus from path: train.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy --output model --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532bcc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK                 100.00\n",
      "TEXTCAT (macro F)   98.85 \n",
      "SPEED               206796\n",
      "\n",
      "\u001b[1m\n",
      "=========================== Textcat F (per label) ===========================\u001b[0m\n",
      "\n",
      "          P       R       F\n",
      "POS   98.63   99.12   98.88\n",
      "NEG   99.08   98.57   98.82\n",
      "\n",
      "\u001b[1m\n",
      "======================== Textcat ROC AUC (per label) ========================\u001b[0m\n",
      "\n",
      "      ROC AUC\n",
      "POS      1.00\n",
      "NEG      1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate ./model/model-best/ ./test.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1be639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
